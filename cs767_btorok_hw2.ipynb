{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f7cd57",
   "metadata": {},
   "source": [
    "In order to accurately predict which of the stocks will be in which performance category we must first do the following:\n",
    "1. Get historical stock price data from the following intervals:\n",
    "4 years, 2 years, 1 year, 6 months, 3 months, 1 month, current price\n",
    "2. We must then get the price of a benchmark (S&P500 chosen here) for the same intervals\n",
    "3. The benchmark log returns should be calculated\n",
    "4. The log returns for each stock should be calculated against the benchmark\n",
    "5. We must then normalize the returns with a z score\n",
    "6. The scores should then be weighted and given a momentum score\n",
    "\n",
    "After the momentum score is given to each stock we can label them as underperform, neutral, overperform based on their momentum scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "9cde7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First create new label based on stock performance relative to other stocks and benchmark\n",
    "# Use the direct path to the file in the environment\n",
    "input_file = 'sp1500_company_info_history.csv'\n",
    "benchmark_input_file = 'benchmark_history.csv'\n",
    "df_sp500 = pd.read_csv(benchmark_input_file)\n",
    "df_raw = pd.read_csv(input_file)\n",
    "\n",
    "benchmark_df = df_sp500[['anchor', 'close']]\n",
    "benchmark_close = benchmark_df.set_index('anchor')['close'].to_dict()\n",
    "\n",
    "# use log return - measurement of investment performance by ratio of final price to initial price\n",
    "def log_return(final, initial):\n",
    "    return np.log(final / initial)\n",
    "\n",
    "# define the benchmark log returns for the S&P500 in the same windows\n",
    "benchmark_returns = {\n",
    "    'r_4y_2y': log_return(benchmark_close['2y'], benchmark_close['4y']),\n",
    "    'r_2y_1y': log_return(benchmark_close['1y'], benchmark_close['2y']),\n",
    "    'r_1y_6m': log_return(benchmark_close['6m'], benchmark_close['1y']),\n",
    "    'r_6m_3m': log_return(benchmark_close['3m'], benchmark_close['6m']),\n",
    "    'r_3m_1m': log_return(benchmark_close['1m'], benchmark_close['3m']),\n",
    "    'r_1m_1d': log_return(benchmark_close['1d'], benchmark_close['1m'])\n",
    "}\n",
    "\n",
    "# get returns for each stock in the same windows, subtract benchmark return to get relative performance\n",
    "# to benchmark\n",
    "r_4y_2y = log_return(df_raw.get('2y_close'), df_raw.get('4y_close')) - benchmark_returns['r_4y_2y']\n",
    "r_2y_1y = log_return(df_raw.get('1y_close'), df_raw.get('2y_close')) - benchmark_returns['r_2y_1y']\n",
    "r_1y_6m = log_return(df_raw.get('6m_close'), df_raw.get('1y_close')) - benchmark_returns['r_1y_6m']\n",
    "r_6m_3m = log_return(df_raw.get('3m_close'), df_raw.get('6m_close')) - benchmark_returns['r_6m_3m']\n",
    "r_3m_1m = log_return(df_raw.get('1m_close'), df_raw.get('3m_close')) - benchmark_returns['r_3m_1m']\n",
    "r_1m_1d = log_return(df_raw.get('currentPrice'), df_raw.get('1m_close')) - benchmark_returns['r_1m_1d']\n",
    "\n",
    "# calculate z score for performance relative to others in each window\n",
    "def zscore(s: pd.Series) -> pd.Series:\n",
    "    mu = s.mean(skipna=True)\n",
    "    sd = s.std(ddof=0, skipna=True)\n",
    "    return (s - mu) / sd\n",
    "\n",
    "# compute z score for all stocks\n",
    "z_4y_2y = zscore(r_4y_2y)\n",
    "z_2y_1y = zscore(r_2y_1y)\n",
    "z_1y_6m = zscore(r_1y_6m)\n",
    "z_6m_3m = zscore(r_6m_3m)\n",
    "z_3m_1m = zscore(r_3m_1m)\n",
    "z_1m_1d = zscore(r_1m_1d)\n",
    "\n",
    "# now create weights dict, more weight for more recent windows apart from 1 month window\n",
    "# gives momentum score, but doesn't break momentum if stock market is in a brief down cycle \n",
    "weights ={\n",
    "   'z_4y_2y': 0.05,\n",
    "   'z_2y_1y': 0.10,\n",
    "   'z_1y_6m': 0.15,\n",
    "   'z_6m_3m': 0.20,\n",
    "   'z_3m_1m': 0.30,\n",
    "   'z_1m_1d': 0.20\n",
    "}\n",
    "# create a dataframe of the z scores\n",
    "z_df = pd.DataFrame({\n",
    "    'z_4y_2y': z_4y_2y,\n",
    "    'z_2y_1y': z_2y_1y,\n",
    "    'z_1y_6m': z_1y_6m,\n",
    "    'z_6m_3m': z_6m_3m,\n",
    "    'z_3m_1m': z_3m_1m,\n",
    "    'z_1m_1d': z_1m_1d\n",
    "})\n",
    "\n",
    "# multiply each z score by respective weights\n",
    "for col, w in weights.items():\n",
    "    z_df[col] = z_df[col] * w\n",
    "\n",
    "# tracks how much weight to include for each row\n",
    "# boolean mask - true for non-NaN cells -> then convert to float and multiply by weights\n",
    "# this gives an actual numeric weight if cell is valid or 0.0 if cell is missing\n",
    "weight_mask = pd.DataFrame({\n",
    "        col: (~z_df[col].isna()).astype(float) * weights[col] for col in z_df.columns\n",
    "    })\n",
    "\n",
    "# calculate the weighted sum of all z-scores for each stock ignoring missing values\n",
    "weighted_sum = z_df.sum(axis=1, skipna=True)\n",
    "# now compute the actual sum of the weights used\n",
    "sum_w = weight_mask.sum(axis=1)\n",
    "\n",
    "# get the weighted average z-score, and normalize so missing data doesn't\n",
    "# lower score unfairly - prevents penalizing stocks with NaN\n",
    "momentum = weighted_sum / sum_w.replace(0, np.nan)\n",
    "\n",
    "# now bin the momentum into percentile ranks\n",
    "pct = momentum.rank(pct=True, method='average') \n",
    "\n",
    "df_with_label = df_raw.copy()\n",
    "# label 0 = underperform, 1 = neurtral, 2 = outperform\n",
    "df_with_label['label'] = pd.cut(pct, \n",
    "                                bins=[-np.inf, 0.33, 0.66, np.inf], \n",
    "                                labels=[0, 1, 2])\n",
    "\n",
    "# drop na values from the label df\n",
    "df_with_label = df_with_label.dropna(subset=['label']).copy()\n",
    "# make the label an int, not categorical or object\n",
    "df_with_label['label'] = df_with_label['label'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "c67c7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2\n",
      "1     1\n",
      "2     1\n",
      "3     2\n",
      "4     0\n",
      "5     0\n",
      "6     2\n",
      "7     2\n",
      "8     1\n",
      "9     2\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    2\n",
      "14    0\n",
      "15    0\n",
      "16    2\n",
      "17    1\n",
      "18    1\n",
      "19    2\n",
      "Name: label, dtype: int32\n",
      "['city', 'state', 'zip', 'country', 'industry', 'industryKey', 'industryDisp', 'sector', 'sectorKey', 'sectorDisp', 'fullTimeEmployees', 'auditRisk', 'boardRisk', 'compensationRisk', 'shareHolderRightsRisk', 'overallRisk', 'governanceEpochDate', 'compensationAsOfEpochDate', 'executiveTeam', 'maxAge', 'priceHint', 'previousClose', 'open', 'dayLow', 'dayHigh', 'regularMarketPreviousClose', 'regularMarketOpen', 'regularMarketDayLow', 'regularMarketDayHigh', 'dividendRate', 'dividendYield', 'exDividendDate', 'payoutRatio', 'fiveYearAvgDividendYield', 'beta', 'trailingPE', 'forwardPE', 'volume', 'regularMarketVolume', 'averageVolume', 'averageVolume10days', 'averageDailyVolume10Day', 'bid', 'ask', 'bidSize', 'askSize', 'marketCap', 'fiftyTwoWeekLow', 'fiftyTwoWeekHigh', 'allTimeHigh', 'allTimeLow', 'priceToSalesTrailing12Months', 'fiftyDayAverage', 'twoHundredDayAverage', 'trailingAnnualDividendRate', 'trailingAnnualDividendYield', 'currency', 'enterpriseValue', 'profitMargins', 'floatShares', 'sharesOutstanding', 'sharesShort', 'sharesShortPriorMonth', 'sharesShortPreviousMonthDate', 'dateShortInterest', 'sharesPercentSharesOut', 'heldPercentInsiders', 'heldPercentInstitutions', 'shortRatio', 'shortPercentOfFloat', 'impliedSharesOutstanding', 'bookValue', 'priceToBook', 'lastFiscalYearEnd', 'nextFiscalYearEnd', 'mostRecentQuarter', 'earningsQuarterlyGrowth', 'netIncomeToCommon', 'trailingEps', 'forwardEps', 'enterpriseToRevenue', 'enterpriseToEbitda', '52WeekChange', 'SandP52WeekChange', 'lastDividendValue', 'lastDividendDate', 'currentPrice', 'targetHighPrice', 'targetLowPrice', 'targetMeanPrice', 'targetMedianPrice', 'recommendationMean', 'recommendationKey', 'numberOfAnalystOpinions', 'totalCash', 'totalCashPerShare', 'ebitda', 'totalDebt', 'quickRatio', 'currentRatio', 'totalRevenue', 'debtToEquity', 'revenuePerShare', 'returnOnAssets', 'returnOnEquity', 'grossProfits', 'freeCashflow', 'operatingCashflow', 'earningsGrowth', 'revenueGrowth', 'grossMargins', 'ebitdaMargins', 'operatingMargins', 'financialCurrency', 'symbol', 'typeDisp', 'earningsTimestamp', 'earningsTimestampStart', 'earningsTimestampEnd', 'earningsCallTimestampStart', 'earningsCallTimestampEnd', 'isEarningsDateEstimate', 'epsTrailingTwelveMonths', 'epsForward', 'epsCurrentYear', 'priceEpsCurrentYear', 'fiftyDayAverageChange', 'fiftyDayAverageChangePercent', 'twoHundredDayAverageChange', 'twoHundredDayAverageChangePercent', 'averageAnalystRating', 'firstTradeDateMilliseconds', 'postMarketChangePercent', 'postMarketPrice', 'postMarketChange', 'regularMarketChange', 'regularMarketDayRange', 'fullExchangeName', 'averageDailyVolume3Month', 'fiftyTwoWeekLowChange', 'fiftyTwoWeekLowChangePercent', 'fiftyTwoWeekRange', 'fiftyTwoWeekHighChange', 'fiftyTwoWeekHighChangePercent', 'fiftyTwoWeekChangePercent', 'dividendDate', 'regularMarketChangePercent', 'postMarketTime', 'regularMarketTime', 'exchange', 'regularMarketPrice', '4y_date', '4y_open', '4y_high', '4y_low', '4y_close', '4y_volume', '4y_vwap', '2y_date', '2y_open', '2y_high', '2y_low', '2y_close', '2y_volume', '2y_vwap', '1y_date', '1y_open', '1y_high', '1y_low', '1y_close', '1y_volume', '1y_vwap', '6m_date', '6m_open', '6m_high', '6m_low', '6m_close', '6m_volume', '6m_vwap', '3m_date', '3m_open', '3m_high', '3m_low', '3m_close', '3m_volume', '3m_vwap', '1m_date', '1m_open', '1m_high', '1m_low', '1m_close', '1m_volume', '1m_vwap', 'label']\n"
     ]
    }
   ],
   "source": [
    "# now add the new label and clean the dataframe by dropping low value columns\n",
    "\n",
    "def clean_data(df):\n",
    "    df_clean = df.copy()\n",
    "    # remove columns that have no value for analysis, have large amount of\n",
    "    # missing data\n",
    "    drop_cols = ['companyOfficers', 'website', 'phone', 'irWebsite',\n",
    "                 'longBusinessSummary', 'address1', 'tradeable', 'quoteType',\n",
    "                 'language', 'region', 'quoteSourceName', 'triggerable', \n",
    "                 'customPriceAlertConfidence', 'marketState',\n",
    "                 'exchangeDataDelayedBy', 'sourceInterval', 'cryptoTradeable',\n",
    "                 'shortName', 'longName', 'hasPrePostMarketData',\n",
    "                 'corporateActions', 'messageBoardId', 'exchangeTimezoneName',\n",
    "                 'exchangeTimezoneShortName', 'gmtOffSetMilliseconds', 'fax',\n",
    "                 'market', 'esgPopulated', 'address2', 'displayName',\n",
    "                 'ipoExpectedDate', 'prevName', 'nameChangeDate',\n",
    "                 'industrySymbol', 'prevTicker', 'tickerChangeDate',\n",
    "                 'trailingPegRatio', 'lastSplitDate', 'lastSplitFactor',\n",
    "                 'massive_error']\n",
    "\n",
    "    df_clean.drop(drop_cols, inplace=True, axis=1)\n",
    "    return df_clean\n",
    "\n",
    "df_cleaned = clean_data(df_with_label)\n",
    "# show label column\n",
    "print(df_cleaned['label'].head(20))\n",
    "# show all columns\n",
    "print(df_cleaned.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "bd8b433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distribution of labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot distribution of labels\n",
    "distribution = df_cleaned['label'].value_counts()\n",
    "distribution.plot.bar()\n",
    "plt.title('Distribution of Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605adfdf",
   "metadata": {},
   "source": [
    "Classes are not imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29683653",
   "metadata": {},
   "source": [
    "Labels are now added to the cleaned dataframe and all unneccesary columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "4e7849c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values must be imputed for neural network, categorical features encoded\n",
    "# first do the test train split as before\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set random seed for tensorflow\n",
    "tf.random.set_seed(90908)\n",
    "\n",
    "features = df_cleaned.drop(columns=['label', 'symbol'])\n",
    "target = df_cleaned['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=44)\n",
    "\n",
    "# convert target labels to numpy array\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "num_features = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# define a pipeline for numerical imputing and min max scaling\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())])  # scale numerical features so no neg values\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# define processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# fit only on train to avoid data leakage\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# use only transform on test data to prevent data leakage\n",
    "# convert X_test to array to flatten for testing\n",
    "X_test_proc = preprocessor.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "2b20c45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00613178 0.06209067 0.         0.         0.00088454 0.0675654\n",
      " 0.04888467 0.         0.2694545  0.         0.05645724 0.\n",
      " 0.03171733 0.08528654 0.08528856 0.         0.1072004  0.02166194\n",
      " 0.14487936 0.07446888 0.         0.         0.06086271 0.\n",
      " 0.04705372 0.         0.12639278 0.1691712  0.         0.\n",
      " 0.         0.         0.24086772 0.17577295 0.07900087 0.21452382\n",
      " 0.02952833 0.09241572 0.00700619 0.19916382 0.01186842 0.\n",
      " 0.         0.06983897 0.         0.27137876 0.11027972 0.13862216\n",
      " 0.48049074 0.1872866  0.         0.         0.         0.05860898\n",
      " 0.         0.15705577 0.         0.3737578  0.01878809 0.07278176\n",
      " 0.07341561 0.         0.         0.28481632 0.         0.02515778\n",
      " 0.06278957 0.         0.03600807 0.04134162 0.19080932 0.21243924\n",
      " 0.         0.         0.         0.10456704 0.         0.\n",
      " 0.21218356 0.         0.19101556 0.         0.06429161 0.\n",
      " 0.15343685 0.08698475 0.         0.         0.00184721 0.03986248\n",
      " 0.05043826 0.07835755 0.05206296 0.         0.10108358 0.\n",
      " 0.04940135 0.         0.11948675 0.18244559 0.         0.26426494\n",
      " 0.09206008 0.02291444 0.0890535  0.04922051 0.         0.15208732\n",
      " 0.02342458 0.         0.02301947 0.03846267 0.06113498 0.20626628\n",
      " 0.00980491 0.23679493 0.13723162 0.         0.         0.\n",
      " 0.08989932 0.02161918 0.04094531 0.         0.33428442 0.01710623\n",
      " 0.19905291 0.06843942]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2198 - loss: 4.4171     \n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4409 - loss: 2.7456\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6094 - loss: 2.1119\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6604 - loss: 1.9518\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6580 - loss: 1.8879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4e5c27970>"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build sequential model by stacking layers\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss=loss_fn,\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_1.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_1.fit(X_train_proc, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "d68514ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 9ms/step - accuracy: 0.5333 - loss: 1.4520\n",
      "Test Loss: 1.4520\n",
      "Test Accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3924d0",
   "metadata": {},
   "source": [
    "Get the results of the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20c519",
   "metadata": {},
   "source": [
    "Now test model performance, first train model with the dense feature in the model parameter and different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5496968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "[0.36870882 0.32604998 0.30524114]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3445 - loss: 1.0959\n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5807 - loss: 0.9232\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7198 - loss: 0.6607\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8520 - loss: 0.3796\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9271 - loss: 0.2279\n",
      "10/10 - 0s - 10ms/step - accuracy: 0.5433 - loss: 1.2081\n",
      "Test Loss: 1.2081\n",
      "Test Accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "# change model to have Dense 3 for the last layer\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_2.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_2.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_2.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_2.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c09578",
   "metadata": {},
   "source": [
    "Placing the dense layer of 3 for the 3 classifications has decreased performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8a358fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "[0.33806136 0.3237887  0.3381499 ]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3088 - loss: 1.1091\n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2753 - loss: 1.1074\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3015 - loss: 1.1006\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3425 - loss: 1.0961\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3664 - loss: 1.0967\n",
      "10/10 - 0s - 10ms/step - accuracy: 0.3467 - loss: 1.0948\n",
      "Test Loss: 1.0948\n",
      "Test Accuracy: 0.3467\n"
     ]
    }
   ],
   "source": [
    "# use the same model as before but compile with adagrad optimizer\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense\n",
    "model_3.compile(optimizer='adagrad',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_3.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_3.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_3.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "00da3340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "[0.39207816 0.3349323  0.27298957]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3220 - loss: 1.1149 \n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3631 - loss: 1.0939\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3816 - loss: 1.0894\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4315 - loss: 1.0792\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4489 - loss: 1.0756\n",
      "10/10 - 0s - 11ms/step - accuracy: 0.5100 - loss: 1.0734\n",
      "Test Loss: 1.0734\n",
      "Test Accuracy: 0.5100\n"
     ]
    }
   ],
   "source": [
    "# use the same model as before but compile with sgd optimizer\n",
    "\n",
    "model_4 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense\n",
    "model_4.compile(optimizer='sgd',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_4.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_4.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_4.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e9e849fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "[0.31244573 0.3310242  0.35653   ]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3820 - loss: 1.0824\n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5852 - loss: 0.8951\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.5732\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9084 - loss: 0.3031\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.1146\n",
      "10/10 - 0s - 10ms/step - accuracy: 0.5967 - loss: 0.9699\n",
      "Test Loss: 0.9699\n",
      "Test Accuracy: 0.5967\n"
     ]
    }
   ],
   "source": [
    "# use the same model as before but compile with nadam optimizer\n",
    "\n",
    "model_5 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense\n",
    "model_5.compile(optimizer='nadam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_5.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_5.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_5.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269ff22",
   "metadata": {},
   "source": [
    "Best results were with model 2 so we will continue with model 2 to refine the parameters used except lower dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c9cf5ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "[0.3566345 0.2871343 0.3562313]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3695 - loss: 1.0953\n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6118 - loss: 0.9075\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7630 - loss: 0.6272\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8414 - loss: 0.4149\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9532 - loss: 0.1872\n",
      "10/10 - 0s - 9ms/step - accuracy: 0.6400 - loss: 0.8539\n",
      "Test Loss: 0.8539\n",
      "Test Accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "# change dropout to 0.15 each time\n",
    "model_6 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_6.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_6.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_6.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_6.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8f825e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "[0.34310395 0.30302897 0.35386714]\n",
      "Epoch 1/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3965 - loss: 1.0862\n",
      "Epoch 2/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6073 - loss: 0.9064\n",
      "Epoch 3/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6887 - loss: 0.7052\n",
      "Epoch 4/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8335 - loss: 0.4487\n",
      "Epoch 5/5\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9110 - loss: 0.2651\n",
      "10/10 - 0s - 9ms/step - accuracy: 0.5667 - loss: 1.0319\n",
      "Test Loss: 1.0319\n",
      "Test Accuracy: 0.5667\n"
     ]
    }
   ],
   "source": [
    "# try with dropout of 0.25 each time\n",
    "model_7 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_7.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "predictions = model_7.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "\n",
    "model_7.fit(X_train_proc, y_train, epochs=5)\n",
    "\n",
    "loss, accuracy = model_7.evaluate(X_test_proc, y_test, verbose=2)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f990be2",
   "metadata": {},
   "source": [
    "Lower dropout gives higher accuracy\n",
    "We will now add some new metrics before going forward with more changes to the model as it seems the performance changes quite drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "69bf5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "[0.3353614  0.32597497 0.3386637 ]\n",
      "10/10 - 0s - 43ms/step - acc: 0.5667 - loss: 1.1269 - prec_neutral: 0.6481 - prec_over: 0.6758 - prec_under: 0.7805 - rec_neutral: 0.2188 - rec_over: 0.6029 - rec_under: 0.1250 - top2_acc: 0.8267\n",
      "Test Loss: 1.1269\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Macro F1: 0.5239\n",
      "Weighted F1: 0.5403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.85      0.39      0.54       104\n",
      "     neutral       0.43      0.30      0.35        83\n",
      "        over       0.54      0.92      0.68       113\n",
      "\n",
      "    accuracy                           0.57       300\n",
      "   macro avg       0.61      0.54      0.52       300\n",
      "weighted avg       0.62      0.57      0.54       300\n",
      "\n",
      "[[ 41  24  39]\n",
      " [  7  25  51]\n",
      " [  0   9 104]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add more metrics\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "metric = tf.keras.metrics\n",
    "\n",
    "model_7 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(3, activation='softmax') # change dense layer to 3 for 3 classes\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_7.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_7.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "print(predictions.flatten())\n",
    "# reduce verbose to 0 so we can see all metrics in output\n",
    "model_7.fit(X_train_proc, y_train, epochs=5, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_7.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_7.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8d6b9",
   "metadata": {},
   "source": [
    "With the new metrics added we can see that our model is predicting neutral for both overperform and underperform, meaning that the model is predicting neutral as a catch-all. However, the model still has an acceptable wighted F1 score and good precision for underperform and overperform. The precision for neutral is just very poor. There are a few things that can cause this: \n",
    "1. Not enough features that are discriminative\n",
    "2. Recent windows are dominating causing mid-range performance to look very similar\n",
    "3. Model is underfitting\n",
    "\n",
    "We will first try to address the under fitting problem to improve performance, then we will change the window weighting and improve our feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "084b6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 40ms/step - acc: 0.5833 - loss: 1.4450 - prec_neutral: 0.5431 - prec_over: 0.6835 - prec_under: 0.8586 - rec_neutral: 0.3938 - rec_over: 0.2647 - rec_under: 0.3320 - top2_acc: 0.8800\n",
      "Test Loss: 1.4450\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Macro F1: 0.5843\n",
      "Weighted F1: 0.5981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.66      0.64      0.65       104\n",
      "     neutral       0.36      0.52      0.43        83\n",
      "        over       0.81      0.58      0.67       113\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.61      0.58      0.58       300\n",
      "weighted avg       0.64      0.58      0.60       300\n",
      "\n",
      "[[67 35  2]\n",
      " [27 43 13]\n",
      " [ 7 41 65]]\n"
     ]
    }
   ],
   "source": [
    "# add more epochs\n",
    "model_8 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_8.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_8.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "# increase epochs to 30, reduce verbose to 0 so other output is not blocked\n",
    "model_8.fit(X_train_proc, y_train, epochs=30, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_8.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_8.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ee8f7",
   "metadata": {},
   "source": [
    "Model performance did not improve significantly, we will decrease epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "26dcd2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 41ms/step - acc: 0.6033 - loss: 1.3165 - prec_neutral: 0.5536 - prec_over: 0.6782 - prec_under: 0.8511 - rec_neutral: 0.3875 - rec_over: 0.2892 - rec_under: 0.3125 - top2_acc: 0.8900\n",
      "Test Loss: 1.3165\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Macro F1: 0.6016\n",
      "Weighted F1: 0.6164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.72      0.65      0.68       104\n",
      "     neutral       0.37      0.51      0.43        83\n",
      "        over       0.77      0.63      0.69       113\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.62      0.60      0.60       300\n",
      "weighted avg       0.64      0.60      0.62       300\n",
      "\n",
      "[[68 33  3]\n",
      " [23 42 18]\n",
      " [ 4 38 71]]\n"
     ]
    }
   ],
   "source": [
    "# Decrease epochs\n",
    "model_9 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_9.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_9.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "# decrease epochs to 20\n",
    "model_9.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_9.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_9.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdca4b",
   "metadata": {},
   "source": [
    "Performance did not significantly change, but more epochs is likely to be less underfit so we will continue with 20 epochs but try a wider model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "53f06fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 42ms/step - acc: 0.6000 - loss: 1.9483 - prec_neutral: 0.5727 - prec_over: 0.7037 - prec_under: 0.8667 - rec_neutral: 0.3938 - rec_over: 0.2794 - rec_under: 0.3555 - top2_acc: 0.8700\n",
      "Test Loss: 1.9483\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.5980\n",
      "Weighted F1: 0.6129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.70      0.70      0.70       104\n",
      "     neutral       0.36      0.49      0.42        83\n",
      "        over       0.80      0.58      0.68       113\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.62      0.59      0.60       300\n",
      "weighted avg       0.64      0.60      0.61       300\n",
      "\n",
      "[[73 29  2]\n",
      " [28 41 14]\n",
      " [ 4 43 66]]\n"
     ]
    }
   ],
   "source": [
    "# use 20 epochs but make model wider by adding an additional layer\n",
    "model_10 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_10.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_10.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_10.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_10.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_10.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf9d68",
   "metadata": {},
   "source": [
    "The performance is slightly improving by adding an additional layer, we will now try with one more additional layer but with less dropout in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6e36451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "10/10 - 0s - 46ms/step - acc: 0.6067 - loss: 2.3420 - prec_neutral: 0.5250 - prec_over: 0.6786 - prec_under: 0.8617 - rec_neutral: 0.3938 - rec_over: 0.2794 - rec_under: 0.3164 - top2_acc: 0.8933\n",
      "Test Loss: 2.3420\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6081\n",
      "Weighted F1: 0.6208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.71      0.64      0.68       104\n",
      "     neutral       0.39      0.57      0.46        83\n",
      "        over       0.80      0.60      0.69       113\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.63      0.60      0.61       300\n",
      "weighted avg       0.66      0.61      0.62       300\n",
      "\n",
      "[[67 34  3]\n",
      " [22 47 14]\n",
      " [ 5 40 68]]\n"
     ]
    }
   ],
   "source": [
    "# add one more additional layer, but have less dropout for each layer\n",
    "model_11 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "model_11.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_11.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_11.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_11.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_11.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630b1fc",
   "metadata": {},
   "source": [
    "Performance with a wider model with less dropout for each layer is improved for the neutral class. We will now explore manually adjusting the learning rate for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e38e492e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 43ms/step - acc: 0.6067 - loss: 1.8001 - prec_neutral: 0.5470 - prec_over: 0.6517 - prec_under: 0.8621 - rec_neutral: 0.4000 - rec_over: 0.2843 - rec_under: 0.2930 - top2_acc: 0.8767\n",
      "Test Loss: 1.8001\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6073\n",
      "Weighted F1: 0.6204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.73      0.63      0.68       104\n",
      "     neutral       0.39      0.55      0.46        83\n",
      "        over       0.78      0.62      0.69       113\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.63      0.60      0.61       300\n",
      "weighted avg       0.65      0.61      0.62       300\n",
      "\n",
      "[[66 33  5]\n",
      " [22 46 15]\n",
      " [ 3 40 70]]\n"
     ]
    }
   ],
   "source": [
    "# manually adjust learning rate\n",
    "model_12 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model_12.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_12.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_12.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_12.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_12.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "8ec31b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "10/10 - 0s - 44ms/step - acc: 0.5833 - loss: 1.7353 - prec_neutral: 0.5760 - prec_over: 0.6835 - prec_under: 0.8478 - rec_neutral: 0.4500 - rec_over: 0.2647 - rec_under: 0.3047 - top2_acc: 0.8933\n",
      "Test Loss: 1.7353\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.5866\n",
      "Weighted F1: 0.5976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.69      0.62      0.65       104\n",
      "     neutral       0.38      0.58      0.46        83\n",
      "        over       0.79      0.56      0.65       113\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.62      0.58      0.59       300\n",
      "weighted avg       0.64      0.58      0.60       300\n",
      "\n",
      "[[64 35  5]\n",
      " [23 48 12]\n",
      " [ 6 44 63]]\n"
     ]
    }
   ],
   "source": [
    "# try even lower learning rate\n",
    "model_13 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
    "model_13.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_13.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_13.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_13.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_13.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe3b38",
   "metadata": {},
   "source": [
    "Weighted F1 did not improve, We will continue to try lower learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "98cb86ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 42ms/step - acc: 0.6300 - loss: 1.0455 - prec_neutral: 0.5234 - prec_over: 0.6562 - prec_under: 0.8488 - rec_neutral: 0.3500 - rec_over: 0.3088 - rec_under: 0.2852 - top2_acc: 0.9100\n",
      "Test Loss: 1.0455\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6261\n",
      "Weighted F1: 0.6413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.75      0.63      0.69       104\n",
      "     neutral       0.40      0.53      0.46        83\n",
      "        over       0.77      0.70      0.73       113\n",
      "\n",
      "    accuracy                           0.63       300\n",
      "   macro avg       0.64      0.62      0.63       300\n",
      "weighted avg       0.66      0.63      0.64       300\n",
      "\n",
      "[[66 34  4]\n",
      " [20 44 19]\n",
      " [ 2 32 79]]\n"
     ]
    }
   ],
   "source": [
    "# try even lower learning rate\n",
    "model_14 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_14.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_14.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_14.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_14.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_14.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ce4f8",
   "metadata": {},
   "source": [
    "Try different activation types in layers to see if there are changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "8f6f0484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 42ms/step - acc: 0.6000 - loss: 1.6636 - prec_neutral: 0.5526 - prec_over: 0.7125 - prec_under: 0.8614 - rec_neutral: 0.3938 - rec_over: 0.2794 - rec_under: 0.3398 - top2_acc: 0.9067\n",
      "Test Loss: 1.6636\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.5998\n",
      "Weighted F1: 0.6134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.68      0.66      0.67       104\n",
      "     neutral       0.38      0.53      0.44        83\n",
      "        over       0.82      0.59      0.69       113\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.62      0.60      0.60       300\n",
      "weighted avg       0.65      0.60      0.61       300\n",
      "\n",
      "[[69 32  3]\n",
      " [27 44 12]\n",
      " [ 6 40 67]]\n"
     ]
    }
   ],
   "source": [
    "activation_leaky = tf.keras.layers.LeakyReLU\n",
    "\n",
    "model_15 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Dense(512, kernel_initializer='he_normal', input_shape=(X_train_proc.shape[1],)),\n",
    "    activation_leaky(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.01),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
    "model_15.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_15.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_15.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_15.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_15.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027e8c7",
   "metadata": {},
   "source": [
    "Performance slightly decreased so we will attempt again with slightly higher alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "820100a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "10/10 - 0s - 46ms/step - acc: 0.6167 - loss: 1.5955 - prec_neutral: 0.5849 - prec_over: 0.6552 - prec_under: 0.8627 - rec_neutral: 0.3875 - rec_over: 0.2794 - rec_under: 0.3438 - top2_acc: 0.8800\n",
      "Test Loss: 1.5955\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.6128\n",
      "Weighted F1: 0.6269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.68      0.67      0.68       104\n",
      "     neutral       0.40      0.52      0.45        83\n",
      "        over       0.80      0.64      0.71       113\n",
      "\n",
      "    accuracy                           0.62       300\n",
      "   macro avg       0.63      0.61      0.61       300\n",
      "weighted avg       0.65      0.62      0.63       300\n",
      "\n",
      "[[70 29  5]\n",
      " [27 43 13]\n",
      " [ 6 35 72]]\n"
     ]
    }
   ],
   "source": [
    "# attempt with slightly higher alpha\n",
    "model_16 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Dense(512, kernel_initializer='he_normal', input_shape=(X_train_proc.shape[1],)),\n",
    "    activation_leaky(alpha=0.02),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.02),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.02),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.02),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
    "model_16.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_16.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_16.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_16.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_16.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b8280",
   "metadata": {},
   "source": [
    "Performance improved compared to the same activation, will try slightly higher alpha again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "f0fb369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "10/10 - 0s - 47ms/step - acc: 0.5933 - loss: 1.7089 - prec_neutral: 0.5431 - prec_over: 0.6552 - prec_under: 0.8632 - rec_neutral: 0.3938 - rec_over: 0.2794 - rec_under: 0.3203 - top2_acc: 0.8800\n",
      "Test Loss: 1.7089\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.5929\n",
      "Weighted F1: 0.6079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.69      0.63      0.66       104\n",
      "     neutral       0.36      0.51      0.42        83\n",
      "        over       0.80      0.62      0.70       113\n",
      "\n",
      "    accuracy                           0.59       300\n",
      "   macro avg       0.62      0.59      0.59       300\n",
      "weighted avg       0.64      0.59      0.61       300\n",
      "\n",
      "[[66 36  2]\n",
      " [25 42 16]\n",
      " [ 5 38 70]]\n"
     ]
    }
   ],
   "source": [
    "# attempt with slightly higher alpha\n",
    "model_17 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Dense(512, kernel_initializer='he_normal', input_shape=(X_train_proc.shape[1],)),\n",
    "    activation_leaky(alpha=0.022),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.022),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.022),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    activation_leaky(alpha=0.022),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
    "model_17.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_17.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_17.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_17.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_17.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ae161",
   "metadata": {},
   "source": [
    "Performance is worse, overall not much difference with this activation method so we will attempt to use a different one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "170e2f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 46ms/step - acc: 0.6000 - loss: 1.4136 - prec_neutral: 0.5778 - prec_over: 0.6979 - prec_under: 0.8358 - rec_neutral: 0.4875 - rec_over: 0.3284 - rec_under: 0.2188 - top2_acc: 0.9333\n",
      "Test Loss: 1.4136\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Macro F1: 0.6036\n",
      "Weighted F1: 0.6158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.79      0.52      0.63       104\n",
      "     neutral       0.38      0.63      0.47        83\n",
      "        over       0.77      0.65      0.71       113\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.65      0.60      0.60       300\n",
      "weighted avg       0.67      0.60      0.62       300\n",
      "\n",
      "[[54 45  5]\n",
      " [14 52 17]\n",
      " [ 0 39 74]]\n"
     ]
    }
   ],
   "source": [
    "model_18 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='gelu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='gelu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='gelu'),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='gelu'),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_18.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_18.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_18.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_18.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_18.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feb586",
   "metadata": {},
   "source": [
    "Performance is the same as relu, we will try adding some regularization and normalization instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6d125aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 46ms/step - acc: 0.6567 - loss: 1.3134 - prec_neutral: 0.6000 - prec_over: 0.6696 - prec_under: 0.8511 - rec_neutral: 0.3000 - rec_over: 0.3775 - rec_under: 0.3125 - top2_acc: 0.8967\n",
      "Test Loss: 1.3134\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.6413\n",
      "Weighted F1: 0.6578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.73      0.67      0.70       104\n",
      "     neutral       0.45      0.47      0.46        83\n",
      "        over       0.75      0.78      0.77       113\n",
      "\n",
      "    accuracy                           0.66       300\n",
      "   macro avg       0.64      0.64      0.64       300\n",
      "weighted avg       0.66      0.66      0.66       300\n",
      "\n",
      "[[70 28  6]\n",
      " [21 39 23]\n",
      " [ 5 20 88]]\n"
     ]
    }
   ],
   "source": [
    "model_19 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(5e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(5e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(5e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(5e-4)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_19.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_19.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "model_19.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_19.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_19.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c48b1",
   "metadata": {},
   "source": [
    "Adding regularization improved the results across the board, we will attempt with an even higher regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4f5ec6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 44ms/step - acc: 0.6633 - loss: 1.4176 - prec_neutral: 0.5588 - prec_over: 0.6458 - prec_under: 0.8586 - rec_neutral: 0.3562 - rec_over: 0.3039 - rec_under: 0.3320 - top2_acc: 0.9333\n",
      "Test Loss: 1.4176\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6575\n",
      "Weighted F1: 0.6728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.75      0.72      0.74       104\n",
      "     neutral       0.43      0.54      0.48        83\n",
      "        over       0.82      0.70      0.76       113\n",
      "\n",
      "    accuracy                           0.66       300\n",
      "   macro avg       0.67      0.65      0.66       300\n",
      "weighted avg       0.69      0.66      0.67       300\n",
      "\n",
      "[[75 28  1]\n",
      " [22 45 16]\n",
      " [ 3 31 79]]\n"
     ]
    }
   ],
   "source": [
    "model_20 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_20.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_20.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_20.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_20.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_20.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3085e8",
   "metadata": {},
   "source": [
    "The L2 regularizer with an even higher regularization has further improved the performance, we will again increase regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "39b4c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 46ms/step - acc: 0.6933 - loss: 1.4443 - prec_neutral: 0.5652 - prec_over: 0.6535 - prec_under: 0.8553 - rec_neutral: 0.3250 - rec_over: 0.4069 - rec_under: 0.2539 - top2_acc: 0.8933\n",
      "Test Loss: 1.4443\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.6780\n",
      "Weighted F1: 0.6933\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.82      0.61      0.70       104\n",
      "     neutral       0.49      0.55      0.52        83\n",
      "        over       0.77      0.88      0.82       113\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.69      0.68      0.68       300\n",
      "weighted avg       0.71      0.69      0.69       300\n",
      "\n",
      "[[63 34  7]\n",
      " [14 46 23]\n",
      " [ 0 14 99]]\n"
     ]
    }
   ],
   "source": [
    "model_21 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_21.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_21.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_21.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_21.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_21.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f0a20",
   "metadata": {},
   "source": [
    "Performance is improved even more, we will try an even higher regularization now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b5074158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "10/10 - 0s - 46ms/step - acc: 0.6767 - loss: 1.5913 - prec_neutral: 0.6296 - prec_over: 0.6636 - prec_under: 0.8646 - rec_neutral: 0.3187 - rec_over: 0.3578 - rec_under: 0.3242 - top2_acc: 0.9300\n",
      "Test Loss: 1.5913\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6601\n",
      "Weighted F1: 0.6778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.76      0.71      0.73       104\n",
      "     neutral       0.45      0.47      0.46        83\n",
      "        over       0.78      0.80      0.79       113\n",
      "\n",
      "    accuracy                           0.68       300\n",
      "   macro avg       0.66      0.66      0.66       300\n",
      "weighted avg       0.68      0.68      0.68       300\n",
      "\n",
      "[[74 26  4]\n",
      " [22 39 22]\n",
      " [ 2 21 90]]\n"
     ]
    }
   ],
   "source": [
    "model_22 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(2e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(2e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(2e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(2e-3)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_22.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_22.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_22.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_22.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_22.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed07d99",
   "metadata": {},
   "source": [
    "The sweet spot seems to be l2 regularization of 1e-3, so we will go forward with that and attempt to further widen the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "7c319333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "10/10 - 0s - 47ms/step - acc: 0.6533 - loss: 1.6333 - prec_neutral: 0.5776 - prec_over: 0.6476 - prec_under: 0.8356 - rec_neutral: 0.4187 - rec_over: 0.3333 - rec_under: 0.2383 - top2_acc: 0.9267\n",
      "Test Loss: 1.6333\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.6489\n",
      "Weighted F1: 0.6643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.77      0.56      0.65       104\n",
      "     neutral       0.42      0.60      0.50        83\n",
      "        over       0.83      0.78      0.80       113\n",
      "\n",
      "    accuracy                           0.65       300\n",
      "   macro avg       0.67      0.65      0.65       300\n",
      "weighted avg       0.70      0.65      0.66       300\n",
      "\n",
      "[[58 44  2]\n",
      " [17 50 16]\n",
      " [ 0 25 88]]\n"
     ]
    }
   ],
   "source": [
    "model_23 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(64, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_23.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_23.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_23.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_23.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_23.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cca4c",
   "metadata": {},
   "source": [
    "Adding an additional layer does not improve the model, we will remove the layer and begin to look at more feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "94e1640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 46ms/step - acc: 0.6733 - loss: 1.3674 - prec_neutral: 0.5698 - prec_over: 0.6475 - prec_under: 0.8471 - rec_neutral: 0.3063 - rec_over: 0.3873 - rec_under: 0.2812 - top2_acc: 0.9233\n",
      "Test Loss: 1.3674\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.6554\n",
      "Weighted F1: 0.6729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.78      0.63      0.70       104\n",
      "     neutral       0.45      0.48      0.47        83\n",
      "        over       0.76      0.85      0.80       113\n",
      "\n",
      "    accuracy                           0.67       300\n",
      "   macro avg       0.66      0.66      0.66       300\n",
      "weighted avg       0.68      0.67      0.67       300\n",
      "\n",
      "[[66 32  6]\n",
      " [18 40 25]\n",
      " [ 1 16 96]]\n"
     ]
    }
   ],
   "source": [
    "model_20 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_20.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_20.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_20.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_20.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_20.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab31f3",
   "metadata": {},
   "source": [
    "We have exhausted our efforts with adjusting the architecture and paramater tuning of our neural network so now we need to look at feature engineering.\n",
    "We will begin by adding some features and creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "53aba4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " currentPrice  fiftyDayAverage  twoHundredDayAverage  dist_ma50  dist_ma200  ma_cross\n",
      "       166.50         157.2214             150.05576   0.059016    0.109588  0.047753\n",
      "        65.99          71.1404              68.28100  -0.072398   -0.033553  0.041877\n",
      "       123.62         131.4586             130.89246  -0.059628   -0.055561  0.004325\n",
      "       218.04         222.3706             199.54900  -0.019475    0.092664  0.114366\n",
      "       250.10         246.7902             296.32050   0.013411   -0.155981 -0.167151\n"
     ]
    }
   ],
   "source": [
    "# add distance to 52 week extremes\n",
    "rng = (df_with_label['fiftyTwoWeekHigh'] - df_with_label['fiftyTwoWeekLow']).replace(0, np.nan)\n",
    "df_with_label['pos_in_52w'] = (df_with_label['currentPrice'] - df_with_label['fiftyTwoWeekLow']) / (rng + 1e-9)\n",
    "\n",
    "# add simple volume surge ratios\n",
    "df_with_label['vol_surge_now_3m']  = df_with_label['volume'] / (df_with_label['averageDailyVolume3Month'] + 1e-9)\n",
    "df_with_label['vol_surge_now_10d'] = df_with_label['volume'] / (df_with_label['averageVolume10days'] + 1e-9)\n",
    "df_with_label['vol_surge_1m_3m']   = df_with_label['1m_volume'] / (df_with_label['averageDailyVolume3Month'] + 1e-9)\n",
    "\n",
    "# add distance to moving averages\n",
    "df_with_label['dist_ma50']  = (df_with_label['currentPrice'] - df_with_label['fiftyDayAverage']) / (df_with_label['fiftyDayAverage'] + 1e-9)\n",
    "df_with_label['dist_ma200'] = (df_with_label['currentPrice'] - df_with_label['twoHundredDayAverage']) / (df_with_label['twoHundredDayAverage'] + 1e-9)\n",
    "df_with_label['ma_cross']   = (df_with_label['fiftyDayAverage'] - df_with_label['twoHundredDayAverage']) / (df_with_label['twoHundredDayAverage'] + 1e-9)\n",
    "\n",
    "df_new_labels = df_with_label[['currentPrice', 'fiftyDayAverage', 'twoHundredDayAverage','dist_ma50', 'dist_ma200', 'ma_cross']].head(5)\n",
    "print(df_new_labels.to_string(index=False))\n",
    "\n",
    "# add liquidity size, shares outstanding ratios\n",
    "df_with_label['log_mcap']  = np.log(df_with_label['marketCap'] + 1)\n",
    "# high outstanding shares = lots of flots is short and performance may be poor\n",
    "df_with_label['float_to_out'] = df_with_label['floatShares'] / (df_with_label['sharesOutstanding'] + 1e-9)\n",
    "\n",
    "# add z scores to the table for neural net to see each z score\n",
    "df_with_label['z_4y_2y'] = z_4y_2y\n",
    "df_with_label['z_2y_1y'] = z_2y_1y\n",
    "df_with_label['z_1y_6m'] = z_1y_6m\n",
    "df_with_label['z_6m_3m'] = z_6m_3m\n",
    "df_with_label['z_3m_1m'] = z_3m_1m\n",
    "df_with_label['z_1m_1d'] = z_1m_1d\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = clean_data(df_with_label)\n",
    "\n",
    "# reprocess\n",
    "features = df_cleaned.drop(columns=['label', 'symbol'])\n",
    "target = df_cleaned['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=44)\n",
    "\n",
    "# convert target labels to numpy array\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "num_features = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# define a pipeline for numerical imputing and min max scaling\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())])  # scale numerical features so no neg values\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# define processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# fit only on train to avoid data leakage\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# use only transform on test data to prevent data leakage\n",
    "# convert X_test to array to flatten for testing\n",
    "X_test_proc = preprocessor.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "bfeb7fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - 48ms/step - acc: 0.7333 - loss: 1.2003 - prec_neutral: 0.5368 - prec_over: 0.6549 - prec_under: 0.8523 - rec_neutral: 0.3187 - rec_over: 0.3627 - rec_under: 0.2930 - top2_acc: 0.9700\n",
      "Test Loss: 1.2003\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Macro F1: 0.7235\n",
      "Weighted F1: 0.7379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.83      0.71      0.77       104\n",
      "     neutral       0.53      0.61      0.57        83\n",
      "        over       0.83      0.84      0.84       113\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.73      0.72      0.72       300\n",
      "weighted avg       0.75      0.73      0.74       300\n",
      "\n",
      "[[74 29  1]\n",
      " [14 51 18]\n",
      " [ 1 17 95]]\n"
     ]
    }
   ],
   "source": [
    "# train neural net with new data\n",
    "model_24 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(8e-4)),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_24.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_24.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_24.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_24.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_24.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8aada",
   "metadata": {},
   "source": [
    "With these additional features we see a huge improvement in the weighted F1 score due to a big increase in precision of both underperformers and neutral classes. However, the neutral class is still struggling the most.\n",
    "We will now try to tune the model one last time with trying logits + from_logits=True for our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e0c76945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendantorok/Documents/Schooling/BU MET/CS767 - Advanced Machine Learning/Assignments/Assignment 2/.venv/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 1s - 52ms/step - acc: 0.7233 - loss: 1.4968 - prec_neutral: 0.5324 - prec_over: 0.6667 - prec_under: 0.8392 - rec_neutral: 0.9250 - rec_over: 0.6176 - rec_under: 0.6523 - top2_acc: 0.9667\n",
      "Test Loss: 1.4968\n",
      "Test Accuracy: 0.5667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Macro F1: 0.7167\n",
      "Weighted F1: 0.7292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.75      0.87      0.80       104\n",
      "     neutral       0.51      0.63      0.57        83\n",
      "        over       0.95      0.66      0.78       113\n",
      "\n",
      "    accuracy                           0.72       300\n",
      "   macro avg       0.74      0.72      0.72       300\n",
      "weighted avg       0.76      0.72      0.73       300\n",
      "\n",
      "[[90 14  0]\n",
      " [27 52  4]\n",
      " [ 3 35 75]]\n"
     ]
    }
   ],
   "source": [
    "# train neural net with new data\n",
    "tf.random.set_seed(5005)\n",
    "model_25 = tf.keras.models.Sequential([\n",
    "    # added l2 regularizer\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(128, activation='relu',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(1e-3)),\n",
    "    tf.keras.layers.Dropout(0.05)\n",
    "])\n",
    "\n",
    "# put together NN with training process, loss, and means of evaluation\n",
    "# use loss of sparse categorical crossentropy since Dense is in layer\n",
    "# adjust learning rate in optimizer manually to have slower learning rate\n",
    "optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_25.compile(optimizer=optimizer,\n",
    "                loss=loss_fn,\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_25.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "model_25.fit(X_train_proc, y_train, epochs=20, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, \n",
    " prec_under, rec_under, \n",
    " prec_neutral, rec_neutral, \n",
    " prec_over, rec_over) = model_25.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_25.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e976bc",
   "metadata": {},
   "source": [
    "We will now use this final model to predict class probabilities and rank overperform probability to get the stocks most likely to overperform in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d01ab3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "     symbol    p_over  confidence recommendationKey  currentPrice   forwardPE\n",
      "398    HOOD  1.000000    1.000000               buy        146.78  201.068480\n",
      "604    CYTK  1.000000    1.000000               buy         63.59  -11.797774\n",
      "1397   SNDK  1.000000    1.000000               buy        199.33   19.709223\n",
      "358    PLTR  1.000000    1.000000              hold        200.47  426.531920\n",
      "314      MU  1.000000    1.000000               buy        223.77   17.386948\n",
      "490     WDC  1.000000    1.000000               buy        150.21   17.049944\n",
      "643     FIX  1.000000    1.000000        strong_buy        965.58   56.832256\n",
      "40      APP  1.000000    1.000000               buy        637.33  123.513570\n",
      "710    KTOS  0.999999    0.999999              none         90.60  153.559330\n",
      "957    ARWR  0.999999    0.999999               buy         42.39  -10.045024\n",
      "1442   TTMI  0.999998    0.999998        strong_buy         67.20   34.111670\n",
      "1471   VSAT  0.999998    0.999998              hold         39.82  -25.044024\n",
      "533    AVAV  0.999998    0.999998        strong_buy        369.91   85.627310\n",
      "1412   STRL  0.999998    0.999998        strong_buy        377.90   60.271133\n",
      "408     STX  0.999997    0.999997               buy        255.88   26.379383\n",
      "281    LRCX  0.999997    0.999997               buy        157.46   36.875880\n",
      "716    LITE  0.999997    0.999997               buy        201.56   54.623306\n",
      "6       AMD  0.999996    0.999996               buy        256.12   50.219610\n",
      "276    KLAC  0.999996    0.999996               buy       1208.74   36.829372\n",
      "577    CIEN  0.999995    0.999995               buy        189.92   69.823524\n"
     ]
    }
   ],
   "source": [
    "# save final manual model\n",
    "import joblib\n",
    "model_25.save(\"nn_model_outperf.h5\")\n",
    "joblib.dump(preprocessor, \"preprocessor.pkl\")\n",
    "\n",
    "final_model = tf.keras.models.load_model('nn_model_outperf.h5')\n",
    "preproc = joblib.load(\"preprocessor.pkl\")\n",
    "\n",
    "df_live = df_cleaned.copy()\n",
    "X_live = preproc.transform(df_live).toarray()\n",
    "\n",
    "# predict probabilities over classes\n",
    "probs = tf.nn.softmax(final_model.predict(X_live), axis=1).numpy()\n",
    "p_under, p_neut, p_over = probs[:,0], probs[:,1], probs[:,2]\n",
    "\n",
    "# rank the scores\n",
    "df_live['p_over'] = p_over\n",
    "# get margin that p_over is greater than the next greatest probability\n",
    "df_live['margin_over_vs_next'] = p_over - np.maximum(p_neut, p_under)\n",
    "# get the confidence interval\n",
    "df_live['confidence'] = df_live[['p_over', 'margin_over_vs_next']].mean(axis=1)\n",
    "\n",
    "# selection rules\n",
    "\n",
    "p_threshold = 0.60\n",
    "stock_picks = df_live.loc[df_live['p_over'] >= p_threshold].copy()\n",
    "top_20 = stock_picks.sort_values('confidence', ascending=False).head(20)\n",
    "\n",
    "# print the top 20 and some key metrics\n",
    "print(top_20[['symbol', 'p_over', 'confidence', 'recommendationKey', 'currentPrice', 'forwardPE']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dc1db",
   "metadata": {},
   "source": [
    "Compare results to the first model\n",
    "Rewrite first model to add all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "3343c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "10/10 - 0s - 47ms/step - acc: 0.5833 - loss: 1.2834 - prec_neutral: 0.5333 - prec_over: 0.6800 - prec_under: 0.8528 - rec_neutral: 1.0000 - rec_over: 1.0000 - rec_under: 0.9961 - top2_acc: 0.9100\n",
      "Test Loss: 1.2834\n",
      "Test Accuracy: 0.5333\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.5871\n",
      "Weighted F1: 0.5958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       under       0.67      0.62      0.64       104\n",
      "     neutral       0.39      0.63      0.48        83\n",
      "        over       0.81      0.52      0.63       113\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.63      0.59      0.59       300\n",
      "weighted avg       0.65      0.58      0.60       300\n",
      "\n",
      "[[64 36  4]\n",
      " [21 52 10]\n",
      " [10 44 59]]\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "     symbol    p_over  confidence recommendationKey  currentPrice   forwardPE\n",
      "604    CYTK  0.999844    0.999770               buy         63.59  -11.797774\n",
      "345    NVDA  0.980852    0.976486        strong_buy        202.49   49.148060\n",
      "358    PLTR  0.936363    0.928822              hold        200.47  426.531920\n",
      "398    HOOD  0.907381    0.897331               buy        146.78  201.068480\n",
      "22     AMZN  0.910354    0.894481        strong_buy        244.22   39.710567\n",
      "1397   SNDK  0.894196    0.887519               buy        199.33   19.709223\n",
      "643     FIX  0.890987    0.877163        strong_buy        965.58   56.832256\n",
      "6       AMD  0.873324    0.855893               buy        256.12   50.219610\n",
      "314      MU  0.862995    0.845795               buy        223.77   17.386948\n",
      "1412   STRL  0.848911    0.833969        strong_buy        377.90   60.271133\n",
      "440    TSLA  0.855728    0.833085              hold        456.56  140.913570\n",
      "734      MP  0.842757    0.825912              none         63.09  525.750000\n",
      "490     WDC  0.829765    0.820142               buy        150.21   17.049944\n",
      "533    AVAV  0.839914    0.818315        strong_buy        369.91   85.627310\n",
      "19    GOOGL  0.827495    0.812971        strong_buy        281.19   31.382812\n",
      "248    INTC  0.828978    0.806020              hold         39.99   41.226803\n",
      "235     HWM  0.820996    0.796880        strong_buy        205.95   64.968450\n",
      "40      APP  0.817941    0.796487               buy        637.33  123.513570\n",
      "1253   MRCY  0.808003    0.793598              none         77.41   83.236565\n",
      "72     AVGO  0.813211    0.790041        strong_buy        369.63   59.907616\n"
     ]
    }
   ],
   "source": [
    "# MUST RE RUN BEGINNING CODE BLOCKS FOR ORIGINAL DATAFRAME AND SPLIT BEFORE NEW FEATURE ENGINEERING FOR THIS TO WORK\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "    # shape should be based on the shape of the train dense matrix\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_proc.shape[1],)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# use the same model except add metrics\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss=loss_fn,\n",
    "                metrics=[metric.SparseCategoricalAccuracy(name='acc'),\n",
    "                         metric.SparseTopKCategoricalAccuracy(k=2, name='top2_acc'),\n",
    "                         # per class percision and recall\n",
    "                         metric.Precision(name='prec_under', class_id=0),\n",
    "                         metric.Recall(name='rec_under', class_id=0),\n",
    "                         metric.Precision(name='prec_neutral', class_id=1),\n",
    "                         metric.Recall(name='rec_neutral', class_id=1),\n",
    "                         metric.Precision(name='prec_over', class_id=2),\n",
    "                         metric.Recall(name='rec_over', class_id=2)\n",
    "                         ])\n",
    "predictions = model_1.predict(X_train_proc[:1])\n",
    "tf.nn.softmax(predictions)\n",
    "\n",
    "# change verbose to be 0 so we can see the whole output\n",
    "model_1.fit(X_train_proc, y_train, epochs=5, verbose=0)\n",
    "\n",
    "# get the evaluation metrics from the model\n",
    "(loss, acc, top2_acc, prec_under, rec_under, prec_neutral, rec_neutral, prec_over, rec_over) = model_1.evaluate(X_test_proc, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "probs = model_1.predict(X_test_proc)\n",
    "pred = probs.argmax(1)\n",
    "\n",
    "macro_f1 = f1_score(y_test, pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test, pred, average='weighted')\n",
    "\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "print(classification_report(y_test, pred, target_names=['under', 'neutral', 'over']))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "\n",
    "# save the model and print the chart\n",
    "model_1.save(\"nn__first_model_outperf.h5\")\n",
    "joblib.dump(preprocessor, \"preprocessor.pkl\")\n",
    "\n",
    "first_model = tf.keras.models.load_model('nn__first_model_outperf.h5')\n",
    "preproc = joblib.load(\"preprocessor.pkl\")\n",
    "\n",
    "df_live = df_cleaned.copy()\n",
    "X_live = preproc.transform(df_live).toarray()\n",
    "\n",
    "# predict probabilities over classes\n",
    "probs = tf.nn.softmax(first_model.predict(X_live), axis=1).numpy()\n",
    "p_under, p_neut, p_over = probs[:,0], probs[:,1], probs[:,2]\n",
    "\n",
    "# rank the scores\n",
    "df_live['p_over'] = p_over\n",
    "df_live['margin_over_vs_next'] = p_over - np.maximum(p_neut, p_under)\n",
    "df_live['confidence'] = df_live[['p_over', 'margin_over_vs_next']].mean(axis=1)\n",
    "\n",
    "# selection rules\n",
    "\n",
    "p_threshold = 0.60\n",
    "stock_picks = df_live.loc[df_live['p_over'] >= p_threshold].copy()\n",
    "top_20 = stock_picks.sort_values('confidence', ascending=False).head(20)\n",
    "\n",
    "# print the top 20 and some key metrics\n",
    "print(top_20[['symbol', 'p_over', 'confidence', 'recommendationKey', 'currentPrice', 'forwardPE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "c2b40ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p over and margin over next\n",
      "     symbol    p_over  margin_over_vs_next\n",
      "604    CYTK  0.998080             0.996282\n",
      "345    NVDA  0.966754             0.960818\n",
      "1397   SNDK  0.915854             0.904881\n",
      "398    HOOD  0.905623             0.890312\n",
      "643     FIX  0.903497             0.886419\n",
      "     symbol  confidence\n",
      "604    CYTK    0.997181\n",
      "345    NVDA    0.963786\n",
      "1397   SNDK    0.910367\n",
      "398    HOOD    0.897967\n",
      "643     FIX    0.894958\n"
     ]
    }
   ],
   "source": [
    "print('p over and margin over next')\n",
    "print(top_20[['symbol', 'p_over', 'margin_over_vs_next']].head(5))\n",
    "print(top_20[['symbol', 'confidence']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3c6a9",
   "metadata": {},
   "source": [
    "Rerun code to get new features back into test train split\n",
    "We will now get a figure of the training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "4d22e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.9492 - loss: 0.4347 - prec_neutral: 0.7151 - prec_over: 0.6916 - prec_under: 0.5267 - rec_neutral: 0.7088 - rec_over: 0.5465 - rec_under: 0.4736 - top2_acc: 0.9841 - val_acc: 0.7767 - val_loss: 1.1724 - val_prec_neutral: 0.5417 - val_prec_over: 0.6524 - val_prec_under: 0.8453 - val_rec_neutral: 0.7312 - val_rec_over: 0.5245 - val_rec_under: 0.5977 - val_top2_acc: 0.9800\n",
      "Epoch 2/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9598 - loss: 0.3889 - prec_neutral: 0.7031 - prec_over: 0.6861 - prec_under: 0.5216 - rec_neutral: 0.7186 - rec_over: 0.5585 - rec_under: 0.4857 - top2_acc: 0.9824 - val_acc: 0.7500 - val_loss: 1.2076 - val_prec_neutral: 0.5336 - val_prec_over: 0.6474 - val_prec_under: 0.8444 - val_rec_neutral: 0.7437 - val_rec_over: 0.4951 - val_rec_under: 0.5938 - val_top2_acc: 0.9833\n",
      "Epoch 3/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9539 - loss: 0.4180 - prec_neutral: 0.7132 - prec_over: 0.6947 - prec_under: 0.5279 - rec_neutral: 0.7204 - rec_over: 0.5451 - rec_under: 0.4831 - top2_acc: 0.9836 - val_acc: 0.7567 - val_loss: 1.2449 - val_prec_neutral: 0.5476 - val_prec_over: 0.6623 - val_prec_under: 0.8424 - val_rec_neutral: 0.7188 - val_rec_over: 0.5000 - val_rec_under: 0.6055 - val_top2_acc: 0.9867\n",
      "Epoch 4/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9568 - loss: 0.4120 - prec_neutral: 0.6969 - prec_over: 0.6915 - prec_under: 0.5434 - rec_neutral: 0.6895 - rec_over: 0.5490 - rec_under: 0.5117 - top2_acc: 0.9818 - val_acc: 0.7567 - val_loss: 1.2193 - val_prec_neutral: 0.5502 - val_prec_over: 0.6667 - val_prec_under: 0.8453 - val_rec_neutral: 0.7188 - val_rec_over: 0.5392 - val_rec_under: 0.5977 - val_top2_acc: 0.9733\n",
      "Epoch 5/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9586 - loss: 0.4001 - prec_neutral: 0.6981 - prec_over: 0.6981 - prec_under: 0.5447 - rec_neutral: 0.6655 - rec_over: 0.5523 - rec_under: 0.5294 - top2_acc: 0.9860 - val_acc: 0.7900 - val_loss: 1.1913 - val_prec_neutral: 0.5498 - val_prec_over: 0.6606 - val_prec_under: 0.8500 - val_rec_neutral: 0.7250 - val_rec_over: 0.5343 - val_rec_under: 0.5977 - val_top2_acc: 0.9800\n",
      "Epoch 6/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9509 - loss: 0.4358 - prec_neutral: 0.7040 - prec_over: 0.6902 - prec_under: 0.5547 - rec_neutral: 0.6965 - rec_over: 0.5477 - rec_under: 0.5163 - top2_acc: 0.9878 - val_acc: 0.7567 - val_loss: 1.2494 - val_prec_neutral: 0.5512 - val_prec_over: 0.6605 - val_prec_under: 0.8432 - val_rec_neutral: 0.7063 - val_rec_over: 0.5245 - val_rec_under: 0.6094 - val_top2_acc: 0.9800\n",
      "Epoch 7/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.9493 - loss: 0.4287 - prec_neutral: 0.7059 - prec_over: 0.7068 - prec_under: 0.5450 - rec_neutral: 0.6744 - rec_over: 0.5562 - rec_under: 0.5257 - top2_acc: 0.9796 - val_acc: 0.7667 - val_loss: 1.2035 - val_prec_neutral: 0.5467 - val_prec_over: 0.6522 - val_prec_under: 0.8508 - val_rec_neutral: 0.7312 - val_rec_over: 0.5147 - val_rec_under: 0.6016 - val_top2_acc: 0.9833\n",
      "Epoch 8/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9632 - loss: 0.3619 - prec_neutral: 0.7126 - prec_over: 0.6920 - prec_under: 0.5478 - rec_neutral: 0.7072 - rec_over: 0.5487 - rec_under: 0.5140 - top2_acc: 0.9882 - val_acc: 0.7733 - val_loss: 1.2224 - val_prec_neutral: 0.5433 - val_prec_over: 0.6647 - val_prec_under: 0.8556 - val_rec_neutral: 0.7063 - val_rec_over: 0.5441 - val_rec_under: 0.6016 - val_top2_acc: 0.9767\n",
      "Epoch 9/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9494 - loss: 0.4244 - prec_neutral: 0.7148 - prec_over: 0.6836 - prec_under: 0.5378 - rec_neutral: 0.6901 - rec_over: 0.5370 - rec_under: 0.4960 - top2_acc: 0.9784 - val_acc: 0.7400 - val_loss: 1.3620 - val_prec_neutral: 0.5482 - val_prec_over: 0.6519 - val_prec_under: 0.8482 - val_rec_neutral: 0.6750 - val_rec_over: 0.5049 - val_rec_under: 0.6328 - val_top2_acc: 0.9767\n",
      "Epoch 10/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9638 - loss: 0.3440 - prec_neutral: 0.6989 - prec_over: 0.7052 - prec_under: 0.5490 - rec_neutral: 0.6382 - rec_over: 0.5640 - rec_under: 0.5426 - top2_acc: 0.9876 - val_acc: 0.7600 - val_loss: 1.2463 - val_prec_neutral: 0.5479 - val_prec_over: 0.6581 - val_prec_under: 0.8508 - val_rec_neutral: 0.7500 - val_rec_over: 0.5000 - val_rec_under: 0.6016 - val_top2_acc: 0.9833\n",
      "Epoch 11/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9497 - loss: 0.4362 - prec_neutral: 0.7136 - prec_over: 0.6826 - prec_under: 0.5408 - rec_neutral: 0.6833 - rec_over: 0.5386 - rec_under: 0.5288 - top2_acc: 0.9767 - val_acc: 0.7500 - val_loss: 1.2740 - val_prec_neutral: 0.5403 - val_prec_over: 0.6538 - val_prec_under: 0.8478 - val_rec_neutral: 0.7125 - val_rec_over: 0.5000 - val_rec_under: 0.6094 - val_top2_acc: 0.9767\n",
      "Epoch 12/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9601 - loss: 0.3557 - prec_neutral: 0.7085 - prec_over: 0.6873 - prec_under: 0.5562 - rec_neutral: 0.6929 - rec_over: 0.5164 - rec_under: 0.5602 - top2_acc: 0.9862 - val_acc: 0.7500 - val_loss: 1.2183 - val_prec_neutral: 0.5479 - val_prec_over: 0.6561 - val_prec_under: 0.8478 - val_rec_neutral: 0.7500 - val_rec_over: 0.5049 - val_rec_under: 0.6094 - val_top2_acc: 0.9867\n",
      "Epoch 13/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9630 - loss: 0.3384 - prec_neutral: 0.7152 - prec_over: 0.6800 - prec_under: 0.5397 - rec_neutral: 0.6940 - rec_over: 0.4955 - rec_under: 0.5542 - top2_acc: 0.9891 - val_acc: 0.7767 - val_loss: 1.2349 - val_prec_neutral: 0.5446 - val_prec_over: 0.6587 - val_prec_under: 0.8462 - val_rec_neutral: 0.6875 - val_rec_over: 0.5392 - val_rec_under: 0.6016 - val_top2_acc: 0.9733\n",
      "Epoch 14/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9560 - loss: 0.3875 - prec_neutral: 0.7104 - prec_over: 0.6986 - prec_under: 0.5511 - rec_neutral: 0.6383 - rec_over: 0.5512 - rec_under: 0.5358 - top2_acc: 0.9820 - val_acc: 0.7633 - val_loss: 1.2305 - val_prec_neutral: 0.5417 - val_prec_over: 0.6582 - val_prec_under: 0.8470 - val_rec_neutral: 0.7312 - val_rec_over: 0.5098 - val_rec_under: 0.6055 - val_top2_acc: 0.9867\n",
      "Epoch 15/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9534 - loss: 0.3838 - prec_neutral: 0.6901 - prec_over: 0.7017 - prec_under: 0.5695 - rec_neutral: 0.6518 - rec_over: 0.5713 - rec_under: 0.5548 - top2_acc: 0.9802 - val_acc: 0.7667 - val_loss: 1.1706 - val_prec_neutral: 0.5475 - val_prec_over: 0.6667 - val_prec_under: 0.8480 - val_rec_neutral: 0.7563 - val_rec_over: 0.5490 - val_rec_under: 0.5664 - val_top2_acc: 0.9867\n",
      "Epoch 16/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9513 - loss: 0.4226 - prec_neutral: 0.7241 - prec_over: 0.7055 - prec_under: 0.5344 - rec_neutral: 0.7292 - rec_over: 0.5935 - rec_under: 0.4747 - top2_acc: 0.9905 - val_acc: 0.7500 - val_loss: 1.2348 - val_prec_neutral: 0.5471 - val_prec_over: 0.6646 - val_prec_under: 0.8457 - val_rec_neutral: 0.7625 - val_rec_over: 0.5147 - val_rec_under: 0.5781 - val_top2_acc: 0.9867\n",
      "Epoch 17/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9501 - loss: 0.4312 - prec_neutral: 0.7102 - prec_over: 0.6709 - prec_under: 0.5357 - rec_neutral: 0.7066 - rec_over: 0.5251 - rec_under: 0.4984 - top2_acc: 0.9780 - val_acc: 0.7700 - val_loss: 1.2372 - val_prec_neutral: 0.5524 - val_prec_over: 0.6667 - val_prec_under: 0.8516 - val_rec_neutral: 0.7250 - val_rec_over: 0.5392 - val_rec_under: 0.6055 - val_top2_acc: 0.9733\n",
      "Epoch 18/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9607 - loss: 0.3709 - prec_neutral: 0.7160 - prec_over: 0.6879 - prec_under: 0.5432 - rec_neutral: 0.6846 - rec_over: 0.5835 - rec_under: 0.5225 - top2_acc: 0.9860 - val_acc: 0.7400 - val_loss: 1.3752 - val_prec_neutral: 0.5505 - val_prec_over: 0.6581 - val_prec_under: 0.8457 - val_rec_neutral: 0.6812 - val_rec_over: 0.5000 - val_rec_under: 0.6211 - val_top2_acc: 0.9767\n",
      "Epoch 19/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9332 - loss: 0.4720 - prec_neutral: 0.7191 - prec_over: 0.6785 - prec_under: 0.5299 - rec_neutral: 0.6619 - rec_over: 0.5372 - rec_under: 0.5008 - top2_acc: 0.9734 - val_acc: 0.7000 - val_loss: 1.5784 - val_prec_neutral: 0.5374 - val_prec_over: 0.6571 - val_prec_under: 0.8462 - val_rec_neutral: 0.7625 - val_rec_over: 0.4510 - val_rec_under: 0.6016 - val_top2_acc: 0.9833\n",
      "Epoch 20/20\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9472 - loss: 0.4289 - prec_neutral: 0.6950 - prec_over: 0.7020 - prec_under: 0.5558 - rec_neutral: 0.6273 - rec_over: 0.5270 - rec_under: 0.5243 - top2_acc: 0.9844 - val_acc: 0.7267 - val_loss: 1.3849 - val_prec_neutral: 0.5435 - val_prec_over: 0.6645 - val_prec_under: 0.8444 - val_rec_neutral: 0.7812 - val_rec_over: 0.4951 - val_rec_under: 0.5938 - val_top2_acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# store model fit in history variable\n",
    "history = model_25.fit(X_train_proc, y_train, epochs=20,verbose=1,\n",
    "                       validation_data=(X_test_proc, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92479a01",
   "metadata": {},
   "source": [
    "Create graph for epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "426ed8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)[['acc', 'loss', 'val_acc']].plot(\n",
    "    figsize=(8, 5), xlim=[0, 10], ylim=[0, 1], grid=True, xlabel='Epoch',\n",
    "    style=[\"r--\", \"g*-\", \"b-\", \"b-*\"]\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
